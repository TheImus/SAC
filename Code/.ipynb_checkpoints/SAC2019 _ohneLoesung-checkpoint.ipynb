{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgaben:\n",
    "\n",
    "Beim Ausführen nicht vergessen die run_id zu verändern, damit man die Ergebnisse vergleichen kann.\n",
    "\n",
    "1) Verändere den Temperatur Wert (alpha) und schaue, wie sich die Ergebnisse verändern. \n",
    "\n",
    "2) Erweitere den Agenten um einen lerndenden Temperture Wert. Implementiere dazu unter Aufgabe 2 für alpha Wert gradient descend. Im Agent ist dazu mit traget_entropie, log_alpha und alpha_optim mögliche Hilfen gegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l38j4LVlQk1l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7gOX2DjREI5"
   },
   "outputs": [],
   "source": [
    "class BasicBuffer:\n",
    "\n",
    "  def __init__(self, max_size):\n",
    "      self.max_size = max_size\n",
    "      self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "      experience = (state, action, np.array([reward]), next_state, done)\n",
    "      self.buffer.append(experience)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "      state_batch = []\n",
    "      action_batch = []\n",
    "      reward_batch = []\n",
    "      next_state_batch = []\n",
    "      done_batch = []\n",
    "\n",
    "      batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "      for experience in batch:\n",
    "          state, action, reward, next_state, done = experience\n",
    "          state_batch.append(state)\n",
    "          action_batch.append(action)\n",
    "          reward_batch.append(reward)\n",
    "          next_state_batch.append(next_state)\n",
    "          done_batch.append(done)\n",
    "\n",
    "      return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuOsdm6hRQp2"
   },
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "\n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "\n",
    "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_pi = log_pi.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wiYqqOVQmM2"
   },
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "  \n",
    "    def __init__(self, env, gamma, tau, alpha, q_lr, policy_lr, a_lr, buffer_maxlen):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.action_range = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "        # hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_step = 0\n",
    "        self.delay_step = 2\n",
    "        \n",
    "        # initialize networks \n",
    "        self.q_net1 = SoftQNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.q_net2 = SoftQNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.target_q_net1 = SoftQNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.target_q_net2 = SoftQNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "\n",
    "        # copy params to target param\n",
    "        for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):\n",
    "            target_param.data.copy_(param)\n",
    "\n",
    "        for target_param, param in zip(self.target_q_net2.parameters(), self.q_net2.parameters()):\n",
    "            target_param.data.copy_(param)\n",
    "\n",
    "        # initialize optimizers \n",
    "        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), lr=q_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q_net2.parameters(), lr=q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "        # entropy temperature. Diese Objekte können für Aufgabe 2 verwendet werden.\n",
    "        self.alpha = alpha\n",
    "        self.target_entropy = -torch.prod(torch.Tensor(self.env.action_space.shape).to(self.device)).item()\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=a_lr)\n",
    "\n",
    "        self.replay_buffer = BasicBuffer(buffer_maxlen)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        mean, log_std = self.policy_net.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        \n",
    "        return self.rescale_action(action)\n",
    "    \n",
    "    def rescale_action(self, action):\n",
    "        return action * (self.action_range[1] - self.action_range[0]) / 2.0 +\\\n",
    "            (self.action_range[1] + self.action_range[0]) / 2.0\n",
    "   \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        dones = dones.view(dones.size(0), -1)\n",
    "        \n",
    "        next_actions, next_log_pi = self.policy_net.sample(next_states)\n",
    "        next_q1 = self.target_q_net1(next_states, next_actions)\n",
    "        next_q2 = self.target_q_net2(next_states, next_actions)\n",
    "        next_q_target = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
    "        expected_q = rewards + (1 - dones) * self.gamma * next_q_target\n",
    "\n",
    "        # q loss\n",
    "        curr_q1 = self.q_net1.forward(states, actions)\n",
    "        curr_q2 = self.q_net2.forward(states, actions)        \n",
    "        q1_loss = F.mse_loss(curr_q1, expected_q.detach())\n",
    "        q2_loss = F.mse_loss(curr_q2, expected_q.detach())\n",
    "\n",
    "        # update q networks        \n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "        \n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "        \n",
    "        # delayed update for policy network and target q networks\n",
    "        new_actions, log_pi = self.policy_net.sample(states)\n",
    "        if self.update_step % self.delay_step == 0:\n",
    "            min_q = torch.min(\n",
    "                self.q_net1.forward(states, new_actions),\n",
    "                self.q_net2.forward(states, new_actions)\n",
    "            )\n",
    "            policy_loss = (self.alpha * log_pi - min_q).mean()\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "            # target networks\n",
    "            for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):\n",
    "                target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)\n",
    "\n",
    "            for target_param, param in zip(self.target_q_net2.parameters(), self.q_net2.parameters()):\n",
    "                target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)\n",
    "\n",
    "        # Aufgabe 2\n",
    "\n",
    "        #Ende Aufgabe 2\n",
    "        self.update_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G29sGsAVQl-s"
   },
   "outputs": [],
   "source": [
    "def mini_batch_train(run_id,env, agent, max_episodes, max_steps, batch_size):\n",
    "    episode_rewards = []\n",
    "    update_step = 0\n",
    "    writer = SummaryWriter(\"./runs/\" + run_id)\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(agent.replay_buffer) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "                update_step += 1\n",
    "\n",
    "            if done or step == max_steps-1:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                writer.add_scalar('episode_reward',episode_reward,episode)\n",
    "                print(\"Episode \" + str(episode) + \": \" + str(episode_reward)) \n",
    "                writer.flush()              \n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC Params\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "#Aufgabe 1\n",
    "alpha = 0.8\n",
    "a_lr = 3e-4\n",
    "q_lr = 3e-4\n",
    "p_lr = 3e-4\n",
    "buffer_maxlen = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiHtp87wRT5Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -882.2469893690885\n",
      "Episode 1: -1071.0734036681934\n",
      "Episode 2: -1445.0881849561933\n",
      "Episode 3: -1538.4513892349885\n",
      "Episode 4: -1165.7530725722668\n",
      "Episode 5: -1117.3509418295735\n",
      "Episode 6: -1503.5490798804262\n",
      "Episode 7: -1190.5727054570334\n",
      "Episode 8: -1218.0715221615535\n",
      "Episode 9: -1514.1547558879083\n",
      "Episode 10: -1158.2968911815449\n",
      "Episode 11: -1262.9835471630897\n",
      "Episode 12: -950.8941681822837\n",
      "Episode 13: -916.9701690640612\n",
      "Episode 14: -750.8139509964824\n",
      "Episode 15: -978.1273758065429\n",
      "Episode 16: -865.9582178684085\n",
      "Episode 17: -861.2565896802265\n",
      "Episode 18: -872.3247677338261\n",
      "Episode 19: -874.2862013185883\n",
      "Episode 20: -792.7309818790524\n",
      "Episode 21: -1027.3535727096266\n",
      "Episode 22: -903.6818764562936\n",
      "Episode 23: -917.4173208233597\n",
      "Episode 24: -903.4282655476172\n",
      "Episode 25: -994.8516615631793\n",
      "Episode 26: -1155.6346426244843\n",
      "Episode 27: -1147.9126493842216\n",
      "Episode 28: -1292.0332675522852\n",
      "Episode 29: -1019.0509347181829\n",
      "Episode 30: -1292.5205744897653\n",
      "Episode 31: -1201.1460939267292\n",
      "Episode 32: -1185.666459793949\n",
      "Episode 33: -1057.6159643598116\n",
      "Episode 34: -834.1983673921948\n",
      "Episode 35: -960.5741582472466\n",
      "Episode 36: -1036.2538117864972\n",
      "Episode 37: -383.102259156973\n",
      "Episode 38: -373.88345927568986\n",
      "Episode 39: -251.5538913372644\n",
      "Episode 40: -757.4930019645534\n",
      "Episode 41: -120.09637833870785\n",
      "Episode 42: -358.91564639176954\n",
      "Episode 43: -370.46873413196397\n",
      "Episode 44: -237.28926879956344\n",
      "Episode 45: -2.3274081256239945\n",
      "Episode 46: -120.76355632491905\n",
      "Episode 47: -129.71723330818472\n",
      "Episode 48: -124.2411900241523\n",
      "Episode 49: -124.29840331439101\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "agent = SACAgent(env, gamma, tau, alpha, q_lr, p_lr, a_lr, buffer_maxlen)\n",
    "run_id = 'run_id'\n",
    "# train\n",
    "episode_rewards = mini_batch_train(run_id,env, agent, 50, 500, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7isB3QMpRfTL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SAC2019.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "bb89f0cceb276c1630f92485392944946aadb4c6c0e4f16bc05894307ee7c700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
