\section{Soft Actor-Critic im kontinuierlichen Raum}
%======SAC Grundprinzip===========
\subsection{SAC Grundprinzip}

\begin{frame}{Kontinuierlicher Aktionsraum}
\begin{itemize}
\item kontinuierliche Aktionsräume benötigen
\item[] $\Rightarrow$ Approximation für Q-Funktion
\item[] $\Rightarrow$ Approximation für Strategie \\[12pt]
\item Schritt von Tabellen zu DNNs
\item Optimierung mittels gradient descent
\end{itemize}
\end{frame}

\begin{frame}{Funktionen und deren Netzwerke}
\begin{itemize}
\item State Value Funktion:
\item[] \textcolor{blue}{$V_{\psi}(s_{t})$} \,\,\,\,\,		$\rightarrow$ Skalar als Ausgabe \\[6pt]
\item Q-Funktion:
\item[] \textcolor{red}{$Q_{\theta}(s_{t},a_{t})$}		$\rightarrow$ Skalar als Ausgabe \\[6pt]
\item Strategie:
\item[] \textcolor{green}{$\pi_{\phi}(s_{t}|a_{t})$} \,	$\rightarrow$ Mittelwert und Kovarianz als Ausgabe $\Rightarrow$ Gauss \\[12pt]
\end{itemize}
Mit Parametervektoren \textcolor{blue}{$\psi$}, \textcolor{red}{$\theta$} und \textcolor{green}{$\phi$}
\end{frame}

%======SAC Update Regeln==========
\subsection{SAC Update Regeln}

\begin{frame}{State Value Funktion}
\begin{itemize}
\item eigenes Netzwetk nicht notwendig, aber
\begin{itemize}
\item stabilisiert Training
\item macht simultanes Training aller Netzwerke möglich \\[12pt]
\end{itemize}
\item somit: Berechnung des state values über eigenes Netzwerk
\end{itemize}
\end{frame}

\begin{frame}{Optimierung State Value Funktion}
\begin{itemize}
\item Minimierung des Fehlers
\item Fehler: Residuenquadratsumme aus state-value und Erwartungswert
\end{itemize}
$J_{V}(\psi)=\mathbb{E}_{s_{t}\sim D}\left[\frac{1}{2}(V_{\psi}(s_{t})-\mathbb{E}_{a_{t}\sim \pi_{\phi}}[Q_{\theta}(s_{t},a_{t})-log \pi_{\phi}(s_{t}|a_{t})])^{2}\right]$ \\[12pt]

$\hat{\nabla}_{\psi}J_{V}(\psi)=\nabla_{\psi}V_{\psi}(s_{t})(V_{\psi}(s_{t})-Q_{\theta}(s_{t},a_{t})+log \pi_{\phi}(s_{t}|a_{t}))$
\end{frame}

\begin{frame}{Optimierung Q-Funktion}
\begin{itemize}
\item Minimierung des Fehlers
\item Fehler: soft Bellman Restwert
\end{itemize}
$J_{Q}(\theta)=\mathbb{E}_{(s_{t},a_{t})\sim D}\left[\frac{1}{2}(Q_{\theta}(s_{t},a_{t})-\hat Q_{\theta}(s_{t},a_{t}))^{2}\right]$ \\[12pt]

mit $\hat Q(s_{t},a_{t})=r(s_{t},a_{t})+\gamma \mathbb{E}_{s_{t+1}\sim p}[V_{\overline{\psi}}(s_{t+1})]$ \\[12pt]

$\hat{\nabla}_{\theta}J_{Q}(\theta)=\nabla_{\theta}Q_{\theta}(a_{t},s_{t})(Q_{\theta}(s_{t},a_{t})-r(s_{t},a_{t})-\gamma V_{\overline{\psi}}(s_{t+1}))$
\end{frame}

\begin{frame}{Optimierung der Strategie}
\begin{itemize}
\item Minimierung des Fehlers
\item Fehler: KL-Divergenz
\end{itemize}
$J_{\pi}(\phi)=\mathbb{E}_{s_{t}\sim D}\left[D_{KL}\left(\pi_{\phi}(\cdot | s_{t})\bigg \vert\bigg \vert\frac{exp(Q_{\theta}(s_{t},\cdot))}{Z_{\theta}(s_{t})}\right)\right]$ \\[12pt]
\begin{itemize}
\item zur Berechnung des Gradienten: reparameterization trick
\end{itemize}
\end{frame}

\begin{frame}{Reparameterization trick (1/2)}
\begin{itemize}
\item Ziel: neue Parameter des Erwartungswertes
\item[] $\Rightarrow$ umschreiben der Zielfunktion und des Gradienten \\[12pt]
\item $f_{\phi}$: vektorwertige Abbildung auf Aktionsraum, mit Parameter $\phi$
\item $\epsilon$: Vektor, aus fixer Vertilungsfunktion, z.B. Gauss \\[12pt]
\end{itemize}
$a_{t}=f_{\phi}(\epsilon_{t};s_{t})$\\[6pt]
neuer Parameter für Erwartungswert: $\mathbb{E}_{s_{t}\sim D} \Rightarrow \mathbb{E}_{s_{t}\sim D,\epsilon_{t}\sim N}$
\end{frame}

\begin{frame}{Reparameterization trick (2/2)}
\begin{itemize}
\item Zielfunktion kann umgeschrieben werden:
\end{itemize}
$J_{\pi}(\phi)=\mathbb{E}_{s_{t}\sim D,\epsilon_{t}\sim N}\left[log\pi_{\phi}(f_{\phi}(\epsilon_{t};s_{t})|s_{t})-Q_{\theta}(s_{t},f_{\phi}(\epsilon_{t};s_{t}))\right]$ \\[12pt]

\begin{itemize}
\item Gradient kann geschätzt werden:
\end{itemize}
$\hat\nabla_{\phi}J_{\pi}(\phi) =$ \\[6pt]
$\nabla_{\phi} log \pi_{\phi}(a_{t}|s_{t}) + (\nabla_{a_{t}}log \pi_{\phi}(a_{t}|s_{t})-\nabla_{a_{t}}Q(s_{t},a_{t}))\nabla_{\phi}f_{\phi}(\epsilon_{t};s_{t})$
\end{frame}

%======SAC Algorithmus============
\subsection{SAC Algorithmus}

\begin{frame}{Algorithmus}
\begin{algorithm}[H]
{\small
\SetAlgoLined
Initialize parameter vectors $\psi$,$\overline{\psi}$,$\theta$,$\phi$\\
\For{each iteration}{
	 \For{each environment step}{
  		$a_{t}\sim \pi_{\phi}(a_{t}|s_{t})$\\
  		$s_{t+1}\sim p(s_{t+1}|s_{t},a_{t})$\\
  		$D \leftarrow D \cup \{(s_{t},a_{t},r(s_{t},a_{t}),s_{t+1})\}$\\
 	}
 	\For{each gradient step}{
		$\psi \leftarrow \psi -\lambda_{V}\hat{\nabla}_{\psi}J_{V}(\psi)$\\
		$\theta_{i} \leftarrow \theta_{i}-\lambda_{Q}\hat{\nabla}_{\theta_{i}}J_{Q}(\theta_{i})$ for $i \in \{1,2\}$ \\
		$\phi \leftarrow \phi - \lambda_{\pi}\hat{\nabla}_{\phi}J_{\pi}(\phi)$\\
		$\overline{\psi} \leftarrow \tau\psi + (1-\tau)\overline{\psi}$\\
	}
}
}
 \caption{Soft Actor-Critic}
\end{algorithm}
\end{frame}
















