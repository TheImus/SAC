\section{Soft Actor-Critic Grundlagen}
\begin{frame}
    \frametitle{Inhaltsverzeichnis}
    \tableofcontents[currentsection]
\end{frame}

\subsection{Motivation}
\begin{frame}{Einordnung}
	\begin{itemize}
		\item Soft Actor-Critic ist ein off-policy reinforcment learning Algorithmus für kontinuierliche Aktions- und Zustandsräume 
		\item Wird in zwei Veröffentlichungen beschrieben:
		\begin{itemize}
			\item Soft Actor-Critic:Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor
			\item Soft Actor-Critic Algorithms and Applications
		\end{itemize}
		\item Besitzt Ähnlichkeiten zu DDPG und TD3 
	\end{itemize} 
\end{frame}

\begin{frame}{Deep Deterministic Policy Gradient (DDPG)}
	\begin{itemize}
		\item Basiert auf Deep Q-Networks
		\item Approximiert mit einem Neuronalen Netz eine Q-Function
		\item Für eine höhere Stabilität wird ein Target Netzwerk genutzt
		\item Nutzt für die Aktionsauswahl einen Actor-Critic Ansatz
	\end{itemize} 
\end{frame}

\begin{frame}{Actor-Critic}
	\begin{itemize}
		\item Soft Actor Critic nutzt einen Actor-Critic Ansatz
		\item Actor lernt eine Policy
		\item Critic lernt eine Value Function
	\end{itemize} 
	\includegraphics[width=120pt]{figures/figtmp34.png}
	\cite{actorcritic}
\end{frame}

\begin{frame}{Twin-Delayed DDPG (TD3)}
	\begin{itemize}
		\item Weiterentwicklung von DDPG
		\item Nutzt zwei Netze für die Schätzung der Q-Values
		\item Q-Value Netze werden öfter upgedatet als andere Netze
		\item Fügt noise zu den target actions hinzu
	\end{itemize} 
\end{frame}

\begin{frame}{Soft Actor-Critic}
	\begin{itemize}
		\item Im Gegensatz zu DDPG und TD3 nutzt Soft Actor-Critic eine stochastische Policy
		\item Durch den off-policy Ansatz besitzt Soft Actor-Critic eine hohe Probeneffizienz 
		\item Die Stabilität kann im Vergleich zu DDPG und TD3 erhöht werden 
	\end{itemize} 
\end{frame}

\begin{frame}{Maximierung der Entropie}
	\begin{itemize}
		\item Standard Reinforcment Lerning maximiert die Belohnung
		\begin{equation}
			\sum_{t=0}^T \mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[{\mathnormal{r}(s_t,a_t)}] \notag
		\end{equation}
		\item Soft Actor-Critic maximiert zusätzlich noch die Entropie
		\begin{equation}
			\sum_{t=0}^T \mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[{\mathnormal{r}(s_t,a_t)} + \alpha \mathcal{H}(\pi(\cdot|s_t))] \notag
		\end{equation}
	\item Parameter $\alpha$ kann fest gesetzt werden oder mittels gradient descent angepasst werden
	\end{itemize} 
\end{frame}

\begin{frame}{Entropie}
	\begin{itemize}
		\item Niedrige Entropie entspricht der Wahl einer festen Aktion
		\item Durch die Maximierung der Entropie haben Aktionen mit ähnlichen Q-Values eine ähnlich Wahrscheinlichkeit ausgewählt zu werden
		\item Die ausgewählte Aktion ist damit möglichst zufällig
	\end{itemize} 
\end{frame}

\subsection{Soft Policy Iteration}
\begin{frame}{Soft Policy Iteration}
	\begin{itemize}
		\item Soft Policy Iteration ist die Basis für Soft Actor-Critic
		\item Benötigt Problem in tabellarischer Form
		\item Evaluiert und verbessert abwechselnd die Policy
	\end{itemize} 
\end{frame}

\begin{frame}{Policy Evaluation}
	\begin{itemize}
		\item Q-Values werden iterativ durch die Anwendung eines modifizierten Bellman backup operators $\mathcal{T}^\pi$ berechnet
		\begin{equation}
			\mathcal{T}^\pi \mathnormal{Q}(s_t,a_t) \triangleq \mathnormal{r}(s_T, a_t) + \gamma \mathbb{E}_{s_t+1\sim \mathnormal{p}}[\mathnormal{V}(s_{t+1})] \notag
		\end{equation}
		\begin{equation}
			\mathnormal{V}(s_{t}) = \mathbb{E}_{a_t \sim \pi}[\mathnormal{Q}(s_t, a_t) - \alpha \, \mathrm{log} \, \pi (a_t|s_t)] \notag
		\end{equation}
		\item Durch wiederholte Ausführung des Operators kann die Q-Function einer Policy $\pi$ berechnet werden
	\end{itemize}
\end{frame}

\begin{frame}{Policy Improvement}
	\begin{itemize}
		\item Policy wird an die neue Q-Funktion angepasst
		\item Mit Hilfe der Kullback-Leibler Divergenz wird die Policy auf eine gaußsche Verteilung projiziert
		\begin{equation}
			\pi_{\mathrm{new}} = \mathrm{arg} \, \min_{\pi^\prime \in \Pi} \mathrm{D_{KL}} \left( \pi^\prime(\cdot | s_t) \parallel \frac{\mathrm{exp} (\frac{1}{\alpha}\mathnormal{Q}^{\pi_{\mathrm{old}}}(s_t, \cdot))}{\mathnormal{Z}^{\pi_{\mathrm{old}}}(s_t)} \right) \notag
		\end{equation}
	\end{itemize} 
\end{frame}
